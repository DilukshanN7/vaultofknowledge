Image Classifiers are AI. They identify and classify objects.
However, LLMs are not.

LLMs play only with languages.
They do not deal with epistemology and connecting language to ideas.

So they can be used to convert written ideas to more easily digestible summaries.
However, they can't be used to infer new information.
# Support
While Stallman calls them "bullshit generator", and I think that's true, it's still good to find if you get something useful out of pure bullshit.

> (I went in two division of thought from this; I really have to figure out how to properly represent writings in this thinking pattern XD. I'll organize them according to the **"broad divisions of thought"**, and **"the order in which I placed them"**, and according to **"the order in which I wrote the lines"**. I don't really remember the order in which I thought the lines XD; I'll also study this condition in [[Representing the Diverse Flow of Thought]])
## Division 2 (Science isn't Always Developed Formally)
4] It's easier for people to say that science was built on proper methods in retrospect.
3] But in reality, many inventions were based on people simply messing around with things.

1] Poetry and beautiful works of Art have often been generated through accidents.
2] Meaning and Beauty don't always overlap.
## Division 1 (Example of Meaningfulness Coming From Bullshit)
5] For example, similar principles have been used to make AI that navigates obstacles or performs some other action, purely based on memory, without a direct knowledge of what that action implies.

6] There's also the question of how 
# Good Alternate Approaches
- Use them like Grammarly does (validate grammar)
- Use them to re-summarize information